# -*- coding: utf-8 -*-
"""CPTS 475 apply local.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1otPVT2kE-t0B7iamVJZbKSE82yV1krBv

Twitter API
"""

import tweepy
#import configparser

import pandas as pd
# import csv
# import re 
# import emoji
# import nltk
# import string
#import preprocessor as p


API_KEY = "[Fill In Here]"
API_KEY_SECRET = "[Fill In Here]"
BEARER_TOKEN = "[Fill In Here]"
ACCESS_TOKEN = "[Fill In Here]"
ACCESS_TOKEN_SECRET = "[Fill In Here]"

client = tweepy.Client(
consumer_key= API_KEY, 
consumer_secret= API_KEY_SECRET, 
bearer_token= BEARER_TOKEN, 
access_token=ACCESS_TOKEN, 
access_token_secret=ACCESS_TOKEN_SECRET)

auth = tweepy.OAuthHandler(client.consumer_key, client.consumer_secret)
auth.set_access_token(client.access_token, client.access_token_secret)
api = tweepy.API(auth, wait_on_rate_limit=True)


keyword = input("Enter keyword (add '#' to search for hashtags): ")
search_word = keyword + " -rt"
num_of_results = input("How Many Results would you like?: ")

tweets = client.search_recent_tweets(query=search_word, tweet_fields=['created_at'], max_results=num_of_results)

columns = ['Time', 'Tweet', 'Sentiment Analysis', 'Sentiment']
data = []
sum = 0
count = 0
score = 0

for tweet in tweets.data:
    score = get_score(tweet.text)
    sum += score
    #print(tweet.text + ": " + str(get_score(tweet.text)))
    data.append([tweet.created_at, tweet.text, score, predict(tweet.text)])
    count = count + 1

average = (sum/count)
df = pd.DataFrame(data, columns=columns)
print("Average Sentiment of " + str(keyword) + ": " + str(average))


#print(df)

#df.to_csv('Sentiment_Analysis_of_{}_Tweets_About_{}.csv'.format(num_of_results, keyword))

"""Analyze"""

import numpy as np
import tensorflow as tf
import time
from tensorflow import keras
from keras_preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
import pickle

# Sentiment
POSITIVE = "POSITIVE"
NEGATIVE = "NEGATIVE"
NEUTRAL = "NEUTRAL"
SENTIMENT_THRESHOLDS = (0.4, 0.7)

SEQUENCE_LENGTH = 300

# Load tokenizer
with open(r'C:\Users\evank\Documents\475\tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

def predict(text, include_neutral=True):
    start_at = time.time()
    # Tokenize text
    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)
    # Predict
    score = model.predict([x_test])[0]
    # Decode sentiment
    label = decode_sentiment(score, include_neutral=include_neutral)

    return {"label": label, "score": float(score),
       "elapsed_time": time.time()-start_at}

def get_score(text):
  start_at = time.time()
  # Tokenize text
  x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)
  # Predict
  return model.predict([x_test])[0][0]

def decode_sentiment(score, include_neutral=True):
    if include_neutral:        
        label = NEUTRAL
        if score <= SENTIMENT_THRESHOLDS[0]:
            label = NEGATIVE
        elif score >= SENTIMENT_THRESHOLDS[1]:
            label = POSITIVE

        return label
    else:
        return NEGATIVE if score < 0.5 else POSITIVE
 
# load model
model = keras.models.load_model(r'C:\Users\evank\Documents\475\sequence.keras')

print(predict("You are ok"))

get_score("")